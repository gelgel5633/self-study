{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"대회명 : Spooky Author Identification\n\n노트북 출처 : https://www.kaggle.com/code/sudalairajkumar/simple-feature-engg-notebook-spooky-author/notebook\n\n대회의 목적\n공용도메인의 spooky 작가에의해 작성된 소설 텍스트가 포함되어있다.\n1. 에드거 앨런 포(EAP)\n2. HP 러브크래프트(HPL)\n3. 메리 울스턴 크래프트(MWS)\n\n목표는 테스트셋에서 문장의 저자를 확인하는 것이다.\n\n이 노트북의 목표:\nspooky 작가를 식별하는데 도움을 주는 다른 특징들을 만드는 것을 목표로 한다.\n처음 단계로, feature engineering을 하기 전 간단한 시각화와 데이터클리닝을 할 것이다.","metadata":{}},{"cell_type":"code","source":"import numpy as np #선형대수\nimport pandas as pd #데이터 처리. csv 파일\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk #natural language tool kit\nfrom nltk.corpus import stopwords\nimport string\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\ncolor = sns.color_palette()\n\n%matplotlib inline\n\neng_stopwords = set(stopwords.words(\"english\"))\npd.options.mode.chained_assignment = None","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:33.673098Z","iopub.execute_input":"2022-04-21T10:59:33.673419Z","iopub.status.idle":"2022-04-21T10:59:35.927887Z","shell.execute_reply.started":"2022-04-21T10:59:33.673383Z","shell.execute_reply":"2022-04-21T10:59:35.926668Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# train, test데이터셋을 읽고 맨 위 몇 라인을 확인한다.\ntrain_df = pd.read_csv(\"../input/spooky-author-identification/train.zip\")\ntest_df = pd.read_csv(\"../input/spooky-author-identification/test.zip\")\nprint(\"train dataset의 수 : \",train_df.shape[0])\nprint(\"test dataset의 수 : \",test_df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:35.930149Z","iopub.execute_input":"2022-04-21T10:59:35.930911Z","iopub.status.idle":"2022-04-21T10:59:36.103828Z","shell.execute_reply.started":"2022-04-21T10:59:35.930860Z","shell.execute_reply":"2022-04-21T10:59:36.102618Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:36.107138Z","iopub.execute_input":"2022-04-21T10:59:36.107458Z","iopub.status.idle":"2022-04-21T10:59:36.127918Z","shell.execute_reply.started":"2022-04-21T10:59:36.107418Z","shell.execute_reply":"2022-04-21T10:59:36.127040Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"각 저자의 출현 횟수를 확인하고 클래스가 균형을 이루고 있는지 확인할 수 있다.","metadata":{}},{"cell_type":"code","source":"cnt_srs = train_df[\"author\"].value_counts() #count_series???\n\nplt.figure(figsize=(8,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel(\"빈도수\", fontsize = 12)\nplt.xlabel(\"작가명\", fontsize = 12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:36.130900Z","iopub.execute_input":"2022-04-21T10:59:36.133109Z","iopub.status.idle":"2022-04-21T10:59:36.370852Z","shell.execute_reply.started":"2022-04-21T10:59:36.133071Z","shell.execute_reply":"2022-04-21T10:59:36.369858Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"불균형이 크지가 않다. 가능하다면 그들의 글쓰기 스타일을 이해하기위해 작가들의 일부 글을 프린트해보자","metadata":{}},{"cell_type":"code","source":"grouped_df = train_df.groupby(\"author\")\nfor name, group in grouped_df:\n    print(\"작가명 : \", name)\n    cnt = 0\n    \n    #iterrows : pandas 함수 중 하나.\n    #행에대해 순환반복을 할 때 사용 => 각 행의 값을 출력할 떄 사용\n    #ind는 index, row는 id를 나타냄\n    for ind, row in group.iterrows():\n        print(row[\"text\"])\n        cnt +=1\n        if cnt ==5:\n            break\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:36.372235Z","iopub.execute_input":"2022-04-21T10:59:36.372528Z","iopub.status.idle":"2022-04-21T10:59:36.391479Z","shell.execute_reply.started":"2022-04-21T10:59:36.372498Z","shell.execute_reply":"2022-04-21T10:59:36.390780Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"현재 텍스트 데이터에 상한한 특수문자가 있다. 이러한 특수문자의 수는 좋은 기능일 수 있고, 아마 나중에 만들 수 있을 것이다\n\n그 외에는 단서가 별로 없다. 이 경우 우리가 만들 수 있는 흥미로운 스타일을 발견했다면(작가의 문체나 글의 특징등을 말하는 듯) 코멘트를 부탁한다.\n\n피쳐 엔지니어링: 두 가지의 메인 파트로 나뉘어진다\n1. meta feature : 단어의 수, 불용어의 수, 구둣점의 수와 같은 텍스트에서 추출된 특성\n2. 텍스트에 기반한 특성 : 빈도, svd, word2vec 등과 같은 텍스트/단어를 직접 기반으로 하는 특성\n\nmeta features:\n메타 기능을 만드는 것부터 시작하여 얼마나 spooky 작가를 잘 예측하는지 볼 것이다.\n목록은 다음과 같다.\n1. 텍스트에서 단어의 수\n2. 텍스트에서 유니크한 단어의 수\n3. 텍스트에서 문자의 수\n4. 불용어의 수\n5. 구둣점의 수\n6. 대문자의 수\n7. 제목케이스의 수\n8. 단어의 평균 길이","metadata":{}},{"cell_type":"code","source":"#텍스트에서 단어의 수\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n\n# 텍스트에서 유니크한 단어의 수 = 중복되지 않는?\ntrain_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n# 텍스트에서 문자의 수\ntrain_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n\n# 텍스트에서 불용어의 수\ntrain_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n# 텍스트에서 구둣점의 수\n# 구둣점을 지우기 위해 string.punctuations를 사용. 정규표현식으로 지우는 방법도 있음\ntrain_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n# 대문자의 수(원래 노트북에는 제목 케이스의 수라고 되어있으나 내용은 대문자의 수 확인)\ntrain_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n# 제목 케이스의 수\n# istitle을 사용하여 각 단어의 첫글자가 대문자면 True, 아니면 Else를 반환\ntrain_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n# 단어의 평균 길이\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:36.392920Z","iopub.execute_input":"2022-04-21T10:59:36.393186Z","iopub.status.idle":"2022-04-21T10:59:38.133750Z","shell.execute_reply.started":"2022-04-21T10:59:36.393157Z","shell.execute_reply":"2022-04-21T10:59:38.132655Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"예측에 도움이 될 몇 가지 새로운 변수를 plot할 것이다","metadata":{}},{"cell_type":"code","source":"train_df[\"num_words\"].loc[train_df[\"num_words\"]>80] = 80 #더 나은 시각화를 위해 자름\nplt.figure(figsize=(12,8))\nsns.violinplot(x=\"author\", y=\"num_words\", data = train_df)\nplt.xlabel(\"Author Name\", fontsize=12)\nplt.ylabel(\"Number of words in text\", fontsize=12)\nplt.title(\"Number of words by author\", fontsize=15) #작가별 단어의 수\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:38.135282Z","iopub.execute_input":"2022-04-21T10:59:38.135899Z","iopub.status.idle":"2022-04-21T10:59:38.458658Z","shell.execute_reply.started":"2022-04-21T10:59:38.135857Z","shell.execute_reply":"2022-04-21T10:59:38.457639Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"eap는 다른 둘에 비해 단어 수가 적은 것 같다","metadata":{}},{"cell_type":"code","source":"train_df[\"num_punctuations\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:38.460415Z","iopub.execute_input":"2022-04-21T10:59:38.460723Z","iopub.status.idle":"2022-04-21T10:59:38.471195Z","shell.execute_reply.started":"2022-04-21T10:59:38.460679Z","shell.execute_reply":"2022-04-21T10:59:38.470183Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#구둣점이 10을 초과하면 그 값을 10으로 설정\ntrain_df[\"num_punctuations\"].loc[train_df[\"num_punctuations\"]>10] = 10 # 나은 시각화를 위해 일부 잘라냄\nplt.figure(figsize=(12,8))\nsns.violinplot(x=\"author\", y=\"num_punctuations\", data= train_df)\nplt.xlabel(\"author name\", fontsize=12)\nplt.ylabel(\"number of punctuations in text\", fontsize=12)\nplt.title(\"number of punctuations by author\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:38.473078Z","iopub.execute_input":"2022-04-21T10:59:38.473640Z","iopub.status.idle":"2022-04-21T10:59:38.956929Z","shell.execute_reply.started":"2022-04-21T10:59:38.473594Z","shell.execute_reply":"2022-04-21T10:59:38.955999Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"이 plot도 다소 유용한 것처럼 보인다. 이제 텍스트 기반의 특징을 만드는 것에 집중할 것이다.\n이러한 메타 기능이 어떻게 도움이 되는지 알아보기위해 기본모델을 구축할 것이다","metadata":{}},{"cell_type":"code","source":"# 모델링을 위한 데이터 준비\nauthor_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\ntrain_y = train_df['author'].map(author_mapping_dict)\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values\n\n# 잘린 변수들을 다시 계산\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\ncols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:38.959750Z","iopub.execute_input":"2022-04-21T10:59:38.960102Z","iopub.status.idle":"2022-04-21T10:59:39.757713Z","shell.execute_reply.started":"2022-04-21T10:59:38.960071Z","shell.execute_reply":"2022-04-21T10:59:39.756778Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"이러한 메타기능만으로 간단한 xgboost를 훈련할 수 있다","metadata":{}},{"cell_type":"code","source":"def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n    return pred_test_y, pred_test_y2, model","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:39.759269Z","iopub.execute_input":"2022-04-21T10:59:39.759684Z","iopub.status.idle":"2022-04-21T10:59:39.772719Z","shell.execute_reply.started":"2022-04-21T10:59:39.759635Z","shell.execute_reply":"2022-04-21T10:59:39.771554Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"커널 실행 시간을 위해 점수에 대한 k폴드 교차검증의 첫번째 폴드만 확인할 수 있습니다.\n로컬에서 실행하는동안 \"중단\"줄을 제거하십시오","metadata":{}},{"cell_type":"code","source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break\nprint(\"cv scores : \", cv_scores)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:39.774439Z","iopub.execute_input":"2022-04-21T10:59:39.775598Z","iopub.status.idle":"2022-04-21T10:59:48.929852Z","shell.execute_reply.started":"2022-04-21T10:59:39.775551Z","shell.execute_reply":"2022-04-21T10:59:48.928851Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"메타 기능만 사용하여 0.987의 mlogloss를 얻었다. 나쁘지 않은 점수이다.\n이러한 기능 중 어떤 것이 중요한지 살펴볼 것이다","metadata":{}},{"cell_type":"code","source":"# 중요변수 plot\nfig, ax = plt.subplots(figsize=(12,12))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:48.933410Z","iopub.execute_input":"2022-04-21T10:59:48.935660Z","iopub.status.idle":"2022-04-21T10:59:49.201327Z","shell.execute_reply.started":"2022-04-21T10:59:48.935612Z","shell.execute_reply":"2022-04-21T10:59:49.200317Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"문자 수, 평균 단어 길이 및 고유 단어 수가 상위 3개 변수인 것으로 나타났다.\n이제 몇 가지 텍스트 기반 기능을 만드는 데 집중할 것이다.\n\n텍스트 기반:\n우리가 만들 수 있는 기본 기능 중 하나는 tf-idf 값이다.","metadata":{}},{"cell_type":"code","source":"# tf-idf vectorizer 변환\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:49.202756Z","iopub.execute_input":"2022-04-21T10:59:49.203107Z","iopub.status.idle":"2022-04-21T10:59:56.214505Z","shell.execute_reply.started":"2022-04-21T10:59:49.203073Z","shell.execute_reply":"2022-04-21T10:59:56.213449Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"tf-idf 벡터를 얻었지만 tricky한 부분이 있다.\ntfidf 출력은 희소행렬이므로 다른 기능과 사용해야하는 경우 몇 가지 선택사항이 있다\n\n1. tfidf에서 상위 n기능을 가져오도록 선택하고(밀도가높은순서), 고밀도 형식으로 변환하여 다른 기능과 연결할 수 있다.\n2. 희소 특성을 이용하여 모델을 작성한 다음 다른 밀집 특성과 함께 특성 중 하나로 예측을 사용한다.\n\n데이터 셋을 기반으로 할 때 하나는 다른 것에 비하면 더 잘 수행될 수 있다. 여기서 tfidf의 모든 기능을 사용하는 아주 좋은 스코어링 커널이 있기 때문에 두 번째 접근방식을 사용할 수 있다.\n\nnaive bayes가 데이터셋에서 나은 성능을 보이는 것 같다. 따라서 tfidf를 사용하여 나이브 베이즈 모델을 구축할 수 있다.(train 하는데 더 빠르다)","metadata":{}},{"cell_type":"code","source":"def runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:56.216153Z","iopub.execute_input":"2022-04-21T10:59:56.216682Z","iopub.status.idle":"2022-04-21T10:59:56.223371Z","shell.execute_reply.started":"2022-04-21T10:59:56.216630Z","shell.execute_reply":"2022-04-21T10:59:56.222197Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"tfidf에서의 나이브 베이즈","metadata":{}},{"cell_type":"code","source":"#행의 갯수가 추가됨\ncopy= train_df.shape[0]\ncopy","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:56.224839Z","iopub.execute_input":"2022-04-21T10:59:56.225179Z","iopub.status.idle":"2022-04-21T10:59:56.242525Z","shell.execute_reply.started":"2022-04-21T10:59:56.225134Z","shell.execute_reply":"2022-04-21T10:59:56.241638Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#np.zeros에서 3은 열의 갯수를 의미하는듯\nppp = np.zeros([copy, 3])\nppp","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:56.244046Z","iopub.execute_input":"2022-04-21T10:59:56.244391Z","iopub.status.idle":"2022-04-21T10:59:56.257526Z","shell.execute_reply.started":"2022-04-21T10:59:56.244350Z","shell.execute_reply":"2022-04-21T10:59:56.256651Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:56.259498Z","iopub.execute_input":"2022-04-21T10:59:56.259986Z","iopub.status.idle":"2022-04-21T10:59:56.738822Z","shell.execute_reply.started":"2022-04-21T10:59:56.259940Z","shell.execute_reply":"2022-04-21T10:59:56.737933Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"tfidf를 사용하여 0.844의 mlogloss = (multiclass log loss)를 얻었다. 메타기능보다 훨씬 좋다.\n혼동행렬을 살펴보겠다.\n\nmlogloss는 3개이상을 분류할때 사용되는 로스값이라고 함","metadata":{}},{"cell_type":"code","source":"#혼동행렬 생성 기능\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\n### From http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py #\ndef plot_confusion_matrix(cm, classes, normalize = False, title = \"Confusion matrix\", cmap = plt.cm.Blues):\n    \"\"\"\n    이 기능은 혼동행렬을 plot과 출력을 한다. 정규화는 normalize=True로 시킬경우 가능하다\n    \"\"\"\n    if normalize:\n        cm = cm.astype(\"float\") / cm.sum(axis = 1)[:,np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    #else:\n    #    print('Confusion matrix, without normalization')\n\n    #print(cm)\n    \n    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:56.740610Z","iopub.execute_input":"2022-04-21T10:59:56.741129Z","iopub.status.idle":"2022-04-21T10:59:56.755054Z","shell.execute_reply.started":"2022-04-21T10:59:56.741082Z","shell.execute_reply":"2022-04-21T10:59:56.753795Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y, axis = 1))\nnp.set_printoptions(precision=2)\n\n# 정규화되지않은 혼동행렬 plot\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=[\"EAP\", \"HPL\", \"MWS\"],\n                     title= \"Confusion matrix, without normalization\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:56.756499Z","iopub.execute_input":"2022-04-21T10:59:56.756743Z","iopub.status.idle":"2022-04-21T10:59:57.059673Z","shell.execute_reply.started":"2022-04-21T10:59:56.756712Z","shell.execute_reply":"2022-04-21T10:59:57.058602Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"많은 요소가 eap로 예측되고 해당클래스에 크게 치우쳐져있다.\n\ntfidf에서 svd\n\ntfidf 벡터가 희소하기 때문에 정보를 압축하고 나타내는 간편한 방법은 svd를 통한 방법이다.\n또한 일반적으로 svd특성은 과거 텍스트기반대회에서 잘 수행되었다.(필자 본인에게)\n따라서 tfidf에 svd기능을 만들어 기능을 추가했다.","metadata":{}},{"cell_type":"code","source":"n_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm=\"arpack\")\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n\ntrain_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","metadata":{"execution":{"iopub.status.busy":"2022-04-21T10:59:57.061344Z","iopub.execute_input":"2022-04-21T10:59:57.061580Z","iopub.status.idle":"2022-04-21T11:00:00.972400Z","shell.execute_reply.started":"2022-04-21T10:59:57.061550Z","shell.execute_reply":"2022-04-21T11:00:00.970995Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"count vectorizer의 나이브베이즈","metadata":{}},{"cell_type":"code","source":"# count vectorizer를 변환\ntfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:00:00.973929Z","iopub.execute_input":"2022-04-21T11:00:00.974597Z","iopub.status.idle":"2022-04-21T11:00:08.590847Z","shell.execute_reply.started":"2022-04-21T11:00:00.974421Z","shell.execute_reply":"2022-04-21T11:00:08.589766Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"이제 count vectorizer 기반 기능을 사용하여 다항 nb 모델을 구축하겠다","metadata":{}},{"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new features #\ntrain_df[\"nb_cvec_eap\"] = pred_train[:,0]\ntrain_df[\"nb_cvec_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_cvec_mws\"] = pred_train[:,2]\ntest_df[\"nb_cvec_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_cvec_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_cvec_mws\"] = pred_full_test[:,2]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:02:25.945980Z","iopub.execute_input":"2022-04-21T11:02:25.946333Z","iopub.status.idle":"2022-04-21T11:02:26.305282Z","shell.execute_reply.started":"2022-04-21T11:02:25.946299Z","shell.execute_reply":"2022-04-21T11:02:26.304175Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y,axis=1))\nnp.set_printoptions(precision=2)\n\n# 정규화하지 않은 혼동행렬 plot\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Confusion matrix of NB on word count, without normalization')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:02:54.191867Z","iopub.execute_input":"2022-04-21T11:02:54.192178Z","iopub.status.idle":"2022-04-21T11:02:54.481197Z","shell.execute_reply.started":"2022-04-21T11:02:54.192146Z","shell.execute_reply":"2022-04-21T11:02:54.480546Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"tfidf vectorizer대신 count vectorizer를 사용하여 0.451의 검증 mlogloss를 얻었다.\n혼동행렬은 이전 것보다 더 좋아졌다\n\ncount vectorizer에서의 나이브 베이즈\n\n데이터 eyeballing에서의 아이디어는 특수문자를 세는 것이 도움이 될 수 있다는 것이다.\n단순히 특수문자를 세는 것보다, 문자 수준에서 count vectorizer를 사용하여 일부 기능을 얻을 수 있다.\n다시 다항식 nb를 사용할 수 있다","metadata":{}},{"cell_type":"code","source":"#tfidf vectorizer 변환\ntfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\n\n# 새로운 특성으로서 예측값 추가\ntrain_df[\"nb_cvec_char_eap\"] = pred_train[:,0]\ntrain_df[\"nb_cvec_char_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_cvec_char_mws\"] = pred_train[:,2]\ntest_df[\"nb_cvec_char_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_cvec_char_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_cvec_char_mws\"] = pred_full_test[:,2]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:07:34.278601Z","iopub.execute_input":"2022-04-21T11:07:34.278962Z","iopub.status.idle":"2022-04-21T11:08:41.888693Z","shell.execute_reply.started":"2022-04-21T11:07:34.278927Z","shell.execute_reply":"2022-04-21T11:08:41.887443Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"교차검증 스코어는 매우 높고 3.75이다.\n그러나, 이것은 단어 수준기능과 다른 정보를 추가할 수 있으므로 최종모델에서도 사용할 것이다. \n\n문자 tfidf vectorizer에서의 나이브 베이즈\n또한 문자 tfidf vectorizer에 대한 나이브 베이즈 예측을 얻을 것이다.","metadata":{}},{"cell_type":"code","source":"#tfidf vectorizer 변환\ntfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new features #\ntrain_df[\"nb_tfidf_char_eap\"] = pred_train[:,0]\ntrain_df[\"nb_tfidf_char_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_tfidf_char_mws\"] = pred_train[:,2]\ntest_df[\"nb_tfidf_char_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_tfidf_char_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_tfidf_char_mws\"] = pred_full_test[:,2]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:10:12.749945Z","iopub.execute_input":"2022-04-21T11:10:12.751019Z","iopub.status.idle":"2022-04-21T11:10:49.653895Z","shell.execute_reply.started":"2022-04-21T11:10:12.750965Z","shell.execute_reply":"2022-04-21T11:10:49.652972Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"문자 tfidf에서의 svd\n또한 문자 tfidf기능에 svd를 생성하고 모델링에 사용할 수 있다","metadata":{}},{"cell_type":"code","source":"n_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n\ntrain_svd.columns = [\"svd_char_\" + str(i) for i in range(n_comp)]\ntest_svd.columns = [\"svd_char_\" + str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:12:40.933318Z","iopub.execute_input":"2022-04-21T11:12:40.933695Z","iopub.status.idle":"2022-04-21T11:12:59.697936Z","shell.execute_reply.started":"2022-04-21T11:12:40.933660Z","shell.execute_reply":"2022-04-21T11:12:59.696773Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"xgboost 모델\nxgboost를 돌리고 결과를 평가할 수 있다","metadata":{}},{"cell_type":"code","source":"cols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break\nprint(\"cv scores : \", cv_scores)\n\nout_df = pd.DataFrame(pred_full_test)\nout_df.columns = ['EAP', 'HPL', 'MWS']\nout_df.insert(0, 'id', test_id)\nout_df.to_csv(\"sub_fe.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:13:13.604958Z","iopub.execute_input":"2022-04-21T11:13:13.606308Z","iopub.status.idle":"2022-04-21T11:13:44.078923Z","shell.execute_reply.started":"2022-04-21T11:13:13.606237Z","shell.execute_reply":"2022-04-21T11:13:44.078051Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"0.3055의 val 점수와 0.32의 lb점수를 얻을 수 있다.\n다시 중요 변수를 체크해보자","metadata":{}},{"cell_type":"code","source":"# 중요한 변수 plot\nfig, ax = plt.subplots(figsize=(12,12))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:14:17.774525Z","iopub.execute_input":"2022-04-21T11:14:17.775443Z","iopub.status.idle":"2022-04-21T11:14:18.792577Z","shell.execute_reply.started":"2022-04-21T11:14:17.775380Z","shell.execute_reply":"2022-04-21T11:14:18.791477Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"나이브 베이즈는 예상대로 최고의 기능이다.\n오분류 오류를 확인하기위해 정오분류표를 구할 것이다","metadata":{}},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y,axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Confusion matrix of XGB, without normalization')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:14:51.824803Z","iopub.execute_input":"2022-04-21T11:14:51.825436Z","iopub.status.idle":"2022-04-21T11:14:52.139828Z","shell.execute_reply.started":"2022-04-21T11:14:51.825392Z","shell.execute_reply":"2022-04-21T11:14:52.138360Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"eap와 mws는 다른 것보다 더 자주 잘못 분류되는 것 같다.\n이 쌍에 대한 예측을 개선하는 기능을 잠재적으로 생성할 수 있다.\n\n이 fe 노트북의 다음 단계\n- 기본 feature에 워드임베딩 사용\n- 다른 meta feature들\n- 문장 감성 분석","metadata":{}},{"cell_type":"markdown","source":"다른 개선사항을 위한 아이디어\n- tfidf 와 count vectorizer 파라미터 튜닝\n- 나이브베이즈와 xgb 모델에 대한 파라미터 튜닝\n- 앙상블 및 다른 모델을 쌓기","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}